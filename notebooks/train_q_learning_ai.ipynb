{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Board Game AI using Q-Learning\n",
    "\n",
    "This notebook is used to train the AI for the board game using Q-learning with PyTorch, enabling GPU acceleration on Windows and maintaining compatibility with Mac. It includes visualizations for better transparency and progress tracking across multiple training sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from board_game_env import BoardGameEnv\n",
    "from q_learning_agent import QLearningAgent\n",
    "from utils import *\n",
    "\n",
    "# Set up logging and create models directory\n",
    "setup_logging()\n",
    "create_models_directory()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the game environment and Q-learning agent\n",
    "env = BoardGameEnv()\n",
    "agent = QLearningAgent(state_size=env.board_size * env.board_size, action_size=env.board_size * env.board_size)\n",
    "\n",
    "# Load the latest model if available\n",
    "load_latest_model(agent, \"models\")\n",
    "\n",
    "# Set training parameters\n",
    "num_episodes = 10000  # Total number of episodes\n",
    "batch_size = 1000  # Number of episodes per batch (analogous to epochs)\n",
    "num_batches = num_episodes // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "\n",
    "Run the training process and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent and collect results\n",
    "trained_agent, rewards, win_rates, batch_win_rates = train_agent(env, agent, num_episodes, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot final training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_results(win_rates, batch_win_rates, agent.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance across versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_version_comparison(env, \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play a test game\n",
    "\n",
    "Let's play a test game to see how the trained AI performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_test_game(env, trained_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}